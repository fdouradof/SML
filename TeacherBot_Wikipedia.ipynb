{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoYJ/vnvuQHyE9JEPt7qkv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fdouradof/SML/blob/main/TeacherBot_Interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usm2lJRcRr3j",
        "outputId": "0fb0956c-eb5b-4bd5-9de3-a48d576c73a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import json, time, os\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def Tfid(docs):\n",
        "    \"\"\" Receive list of strings\n",
        "        returns list of tfid matrices for each string\n",
        "        returns also list of keywords as a hyperparameter\"\"\"\n",
        "\n",
        "    docs = [str(x).lower() for x in docs]\n",
        "    vectorizer = TfidfVectorizer(max_features=10, min_df=0.01, max_df=0.99, stop_words=stopwords.words('english'))\n",
        "    X = vectorizer.fit_transform(docs).toarray()\n",
        "    key_words = vectorizer.get_feature_names_out()\n",
        "    return X, key_words\n",
        "\n",
        "def catalog(url,subjects):\n",
        "    \"\"\" Receives URL and returns list of links\n",
        "        with title matching any subject as pandas dataframe\"\"\"\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    SIZE = 10\n",
        "    url_links = []\n",
        "    url_texts = []\n",
        "    for link in soup.find_all(\"a\"):\n",
        "        url = link.get(\"href\", \"\")\n",
        "        for subject in subjects:\n",
        "            if subject in url.lower() and len(url_links) < SIZE+1:\n",
        "                url_links.append(url)\n",
        "                url_texts.append(link.text.strip())\n",
        "    catalogue = pd.DataFrame({\"Title\": url_texts, \"Link\": url_links})\n",
        "    return catalogue\n",
        "\n",
        "def wiki_reader(catalog,kws):\n",
        "    \n",
        "    \"\"\"NEED FIXING THE DUPLICATES BEING ADDED DO DF\"\"\"\n",
        "    \n",
        "    \"\"\"\"reads links from Catalog dataframe\n",
        "        find paragraphs on links with matches from kws\n",
        "        returns name of page, kw match, regex treated text and tfid with keywords\n",
        "        as pandas dataframe\n",
        "    \"\"\"\n",
        "    catalog = catalog.drop_duplicates()              # dont visit sites more than once\n",
        "    data = []\n",
        "    keys = []\n",
        "    matched = False\n",
        "    for k in range(len(catalog)):\n",
        "        url = \"https://en.wikipedia.org\" + catalog.iloc[k][\"Link\"]\n",
        "        try:\n",
        "            page = requests.get(url)\n",
        "        except:\n",
        "            continue\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "        page_title = soup.find(\"span\", class_=\"mw-page-title-main\")\n",
        "        page_texts = soup.find_all(\"p\")\n",
        "        # print(\"Analizing Page title:     \", page_title.text)\n",
        "\n",
        "        for paragraph in page_texts:\n",
        "            rg_parag = dore(paragraph)\n",
        "            tk_parag = nltk.word_tokenize(paragraph.text)\n",
        "            for kw in kws:\n",
        "                if kw in tk_parag:\n",
        "                    matched = True\n",
        "                    key_found = kw\n",
        "            if matched:\n",
        "                try:\n",
        "                    X, key_words = Tfid(tk_parag)\n",
        "                    data.append((page_title, rg_parag, key_words, X, keys))\n",
        "                    keys.append(key_found)\n",
        "                except:\n",
        "                    pass\n",
        "                    \n",
        "    DATA = pd.DataFrame(data, columns=[\"page_title\", \"page_text\", \"keywords\", \"tfid\",\"match\"])\n",
        "    return DATA\n",
        "\n",
        "def dore(paragraph):\n",
        "\n",
        "    \"\"\" receives one or more sentences in one string\n",
        "        returns the clean sentences joined by a space\n",
        "    \"\"\"\n",
        "    joined_sentences = []\n",
        "    sentences = nltk.sent_tokenize(paragraph.text)\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = sentences[i].lower()\n",
        "        sentences[i] = re.sub(r'[^A-Za-z0-9,. ]+', ' ', sentences[i])\n",
        "        sentences[i] = re.sub(r'\\W', ' ', sentences[i])  # Substitute the non-alphanumeric characters with space.\n",
        "        sentences[i] = re.sub(r'\\s+', ' ', sentences[i])  # Remove the excess of white spaces.\n",
        "        sentences[i] = re.sub(r'\\s$', '', sentences[i])  # Remove the space at the end of a sentence.\n",
        "        sentences[i] = re.sub(r'also', '', sentences[i])\n",
        "        sentences[i] = re.sub(r'displaystyle', '', sentences[i])\n",
        "        sentences[i] = re.sub(r',', '', sentences[i])\n",
        "        sentences[i] = re.sub(r'also', '', sentences[i])\n",
        "    for i in range(len(sentences)):\n",
        "        sentences[i] = nltk.word_tokenize(sentences[i])  # Tokenize into words.\n",
        "        sentences[i] = [x for x in sentences[i] if x not in stopwords.words('english')]  # Remove the stop words.\n",
        "        sentences[i] = ' '.join(sentences[i])\n",
        "    joined_sentences.append(sentences)\n",
        "    return joined_sentences\n",
        "def Google(query):\n",
        "    \"\"\"Get first 10 results from query and return a pandas dataframe with\n",
        "            'title'\n",
        "            \"URL\"\n",
        "            'snippet\n",
        "            \"Long description\n",
        "        API allows for more options if needed\n",
        "\"\"\"\n",
        "    google_data = []\n",
        "    API_KEY = \"AIzaSyBO5ZaX_GdfgkYCILF18h4p601JhBzetco\"\n",
        "    SEARCH_ENGINE_ID = \"82bccba347bf948a3\"\n",
        "    # SEARCH_ENGINE_ID = \"8606138438d1d4ebc\" # Secondary ID if needed\n",
        "    page = 1\n",
        "    start = (page - 1) * 10 + 1\n",
        "    url = f\"https://www.googleapis.com/customsearch/v1?key={API_KEY}&cx={SEARCH_ENGINE_ID}&q={query}&start={start}\"\n",
        "    data = requests.get(url).json()\n",
        "    search_items = data.get(\"items\")\n",
        "    for i, search_item in enumerate(search_items, start=1): # when error here means that API reached short/long term limit\n",
        "        try:\n",
        "            long_description = search_item[\"pagemap\"][\"metatags\"][0][\"og:description\"]\n",
        "        except KeyError:\n",
        "            long_description = \"N/A\"\n",
        "        title = search_item.get(\"title\")\n",
        "        snippet = search_item.get(\"snippet\")\n",
        "        link = search_item.get(\"link\")\n",
        "        google_data.append({\n",
        "            'title': title,\n",
        "            \"URL\": link,\n",
        "            'snippet': snippet,\n",
        "            \"long description\": long_description,\n",
        "        })\n",
        "    search_result = pd.read_json(json.dumps(google_data, indent=2, ensure_ascii=False))\n",
        "    return search_result\n",
        "\n",
        "def save_results(search_results):\n",
        "    \"\"\"save result as tuple in all_searches list\"\"\"\n",
        "    \"\"\"BROKEN\"\"\"\n",
        "    X, data_kws = Tfid(search_results[\"page_text\"])\n",
        "    print(\"PESQUISA EM\", url, \"KWS\", data_kws)\n",
        "    sresult = (search_results, X, data_kws)\n",
        "    all_searches.append(sresult)\n",
        "    print(\"PESQUISA GUARDADA\")\n",
        "    return all_searches\n",
        "def update_with_google(pesquisa,all_searches):\n",
        "    \"\"\"building\"\"\"\n",
        "    for i in range(1):\n",
        "        x, kws = Tfid(pesquisa[\"snippet\"])\n",
        "        new_subjects = kws\n",
        "        print(\"USED KEYS\", kws)\n",
        "        catalog2 = catalog(url,new_subjects)\n",
        "        search_data = wiki_reader(catalog2,KEYWORDS)\n",
        "        all_searches = save_results(search_data)\n",
        "    return all_searches\n",
        "def update_with_wiki(search_data):\n",
        "    \"\"\"building\"\"\"\n",
        "    for i in range(2):\n",
        "        print(\"NOVAS CHAVES\", search_data[\"keywords\"][0])\n",
        "        new_subjects = search_data[\"keywords\"][0]\n",
        "        catalog2 = catalog(url,new_subjects)\n",
        "        search_data = wiki_reader(catalog2,KEYWORDS)\n",
        "        all_searches = save_results(search_data)\n",
        "    return all_searches\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "START_WORD = \"chemistry\"\n",
        "SUBJECTS = [\"biology\",\"chemistry\",\"physics\",\"engineering\"]\n",
        "KEYWORDS = [\"mechanical\", \"science\", \"scientific\",\"quantum\",\"particle\"]"
      ],
      "metadata": {
        "id": "aq6sGtuaR3JP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pesquisa = Google(\"branches of \" + START_WORD)\n",
        "url = \"https://en.wikipedia.org/wiki/\" + START_WORD.lower()\n",
        "catalog1 = catalog(url, SUBJECTS)\n",
        "search_data = wiki_reader(catalog1, KEYWORDS)\n"
      ],
      "metadata": {
        "id": "9R-Aeh4qR33j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(search_data.head(5))\n",
        "#needs to fix duplicates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "-nV6PdWeR5fi",
        "outputId": "180523ee-7aa0-4d95-9195-c4533b9e16a9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                      page_title  \\\n",
              "0   [Chemistry (disambiguation)]   \n",
              "1   [Chemistry (disambiguation)]   \n",
              "2  [Index of chemistry articles]   \n",
              "3  [Index of chemistry articles]   \n",
              "4         [Outline of chemistry]   \n",
              "\n",
              "                                           page_text  \\\n",
              "0  [[chemistry branch physical science study subs...   \n",
              "1                            [[chemistry may refer]]   \n",
              "2  [[chemistry egyptian k chem meaning earth 1 ph...   \n",
              "3  [[list chemistry related articles, chemical co...   \n",
              "4  [[following outline provided overview topical ...   \n",
              "\n",
              "                                            keywords  \\\n",
              "0  [branch, chemistry, composed, matter, physical...   \n",
              "1                      [also, chemistry, may, refer]   \n",
              "2  [changes, egyptian, kēme, matter, meaning, phy...   \n",
              "3  [articles, biomolecules, chemical, chemistry, ...   \n",
              "4  [chemistry, following, guide, outline, overvie...   \n",
              "\n",
              "                                                tfid  \\\n",
              "0  [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0....   \n",
              "1  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...   \n",
              "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....   \n",
              "\n",
              "                                               match  \n",
              "0  [science, science, science, science, science, ...  \n",
              "1  [science, science, science, science, science, ...  \n",
              "2  [science, science, science, science, science, ...  \n",
              "3  [science, science, science, science, science, ...  \n",
              "4  [science, science, science, science, science, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e8e9487-c25e-41a0-9bf4-de0c86496289\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_title</th>\n",
              "      <th>page_text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>tfid</th>\n",
              "      <th>match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Chemistry (disambiguation)]</td>\n",
              "      <td>[[chemistry branch physical science study subs...</td>\n",
              "      <td>[branch, chemistry, composed, matter, physical...</td>\n",
              "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0....</td>\n",
              "      <td>[science, science, science, science, science, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Chemistry (disambiguation)]</td>\n",
              "      <td>[[chemistry may refer]]</td>\n",
              "      <td>[also, chemistry, may, refer]</td>\n",
              "      <td>[[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...</td>\n",
              "      <td>[science, science, science, science, science, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Index of chemistry articles]</td>\n",
              "      <td>[[chemistry egyptian k chem meaning earth 1 ph...</td>\n",
              "      <td>[changes, egyptian, kēme, matter, meaning, phy...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[science, science, science, science, science, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Index of chemistry articles]</td>\n",
              "      <td>[[list chemistry related articles, chemical co...</td>\n",
              "      <td>[articles, biomolecules, chemical, chemistry, ...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[science, science, science, science, science, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Outline of chemistry]</td>\n",
              "      <td>[[following outline provided overview topical ...</td>\n",
              "      <td>[chemistry, following, guide, outline, overvie...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1....</td>\n",
              "      <td>[science, science, science, science, science, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e8e9487-c25e-41a0-9bf4-de0c86496289')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e8e9487-c25e-41a0-9bf4-de0c86496289 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e8e9487-c25e-41a0-9bf4-de0c86496289');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
